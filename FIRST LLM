{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accc9ff2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-01T04:37:53.895034Z",
     "iopub.status.busy": "2026-02-01T04:37:53.894663Z",
     "iopub.status.idle": "2026-02-01T04:37:55.093760Z",
     "shell.execute_reply": "2026-02-01T04:37:55.092435Z"
    },
    "papermill": {
     "duration": 1.205892,
     "end_time": "2026-02-01T04:37:55.096068",
     "exception": false,
     "start_time": "2026-02-01T04:37:53.890176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9351ab8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T04:37:55.102705Z",
     "iopub.status.busy": "2026-02-01T04:37:55.101603Z",
     "iopub.status.idle": "2026-02-01T04:38:00.782735Z",
     "shell.execute_reply": "2026-02-01T04:38:00.781338Z"
    },
    "papermill": {
     "duration": 5.686715,
     "end_time": "2026-02-01T04:38:00.785007",
     "exception": false,
     "start_time": "2026-02-01T04:37:55.098292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d89a42d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T04:38:00.792589Z",
     "iopub.status.busy": "2026-02-01T04:38:00.792142Z",
     "iopub.status.idle": "2026-02-01T04:38:05.696885Z",
     "shell.execute_reply": "2026-02-01T04:38:05.695747Z"
    },
    "papermill": {
     "duration": 4.911421,
     "end_time": "2026-02-01T04:38:05.699080",
     "exception": false,
     "start_time": "2026-02-01T04:38:00.787659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 27\n",
      "Vocabulary: \n",
      " ',:OTWabdefghiklmnoqrstuw\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 1. The Dataset\n",
    "# A tiny sample text (Shakespearian style)\n",
    "text = \"\"\"\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "\"\"\"\n",
    "\n",
    "# Get all unique characters (our \"vocabulary\")\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b334c6d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T04:38:05.705931Z",
     "iopub.status.busy": "2026-02-01T04:38:05.705403Z",
     "iopub.status.idle": "2026-02-01T04:38:05.765025Z",
     "shell.execute_reply": "2026-02-01T04:38:05.764142Z"
    },
    "papermill": {
     "duration": 0.065427,
     "end_time": "2026-02-01T04:38:05.767194",
     "exception": false,
     "start_time": "2026-02-01T04:38:05.701767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 encoded tokens: tensor([ 0,  6, 20,  1,  9, 11,  3,  1, 20, 22])\n"
     ]
    }
   ],
   "source": [
    "# Create mappings from character to integer and vice versa\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder: string -> list of ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: list of ints -> string\n",
    "\n",
    "# Test it\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"First 10 encoded tokens: {data[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deca6473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T04:38:05.775022Z",
     "iopub.status.busy": "2026-02-01T04:38:05.773920Z",
     "iopub.status.idle": "2026-02-01T04:38:05.783283Z",
     "shell.execute_reply": "2026-02-01T04:38:05.782259Z"
    },
    "papermill": {
     "duration": 0.015579,
     "end_time": "2026-02-01T04:38:05.785296",
     "exception": false,
     "start_time": "2026-02-01T04:38:05.769717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # B = Batch (how many parallel sequences)\n",
    "        # T = Time (length of the sequence)\n",
    "        \n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) where C is vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape for CrossEntropyLoss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # Calculate how well we predicted the next character\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # Focus only on the last time step (the last character)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28de5cdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T04:38:05.793208Z",
     "iopub.status.busy": "2026-02-01T04:38:05.792245Z",
     "iopub.status.idle": "2026-02-01T04:38:31.231329Z",
     "shell.execute_reply": "2026-02-01T04:38:31.230319Z"
    },
    "papermill": {
     "duration": 25.445509,
     "end_time": "2026-02-01T04:38:31.233623",
     "exception": false,
     "start_time": "2026-02-01T04:38:05.788114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Step 0: Loss 3.8009\n",
      "Step 1000: Loss 2.6583\n",
      "Step 2000: Loss 2.1055\n",
      "Step 3000: Loss 1.8582\n",
      "Step 4000: Loss 1.6526\n",
      "Step 5000: Loss 1.6175\n",
      "Step 6000: Loss 1.6025\n",
      "Step 7000: Loss 1.6521\n",
      "Step 8000: Loss 1.5264\n",
      "Step 9000: Loss 1.5256\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "batch_size = 32\n",
    "block_size = 8 # Context length\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for steps in range(10000): # High number of steps for such a small dataset\n",
    "    \n",
    "    # 1. Get a batch of data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    # 2. Forward pass\n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    # 3. Backward pass (Update weights)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"Step {steps}: Loss {loss.item():.4f}\")\n",
    "\n",
    "print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bf1652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T04:38:31.242208Z",
     "iopub.status.busy": "2026-02-01T04:38:31.241695Z",
     "iopub.status.idle": "2026-02-01T04:38:31.272580Z",
     "shell.execute_reply": "2026-02-01T04:38:31.271519Z"
    },
    "papermill": {
     "duration": 0.0374,
     "end_time": "2026-02-01T04:38:31.274456",
     "exception": false,
     "start_time": "2026-02-01T04:38:31.237056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATED TEXT ---\n",
      "\n",
      "Thems orowsta tr ior be,\n",
      "Orrrot a s arowslind sublind tis n:\n",
      "Whe bertis oust quf s t aro sutof t aga\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- GENERATED TEXT ---\")\n",
    "# Start with a single zero token (index 0)\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_ids = model.generate(context, max_new_tokens=100)\n",
    "print(decode(generated_ids[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.666966,
   "end_time": "2026-02-01T04:38:33.937559",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-01T04:37:50.270593",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
