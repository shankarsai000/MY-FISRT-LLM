{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T04:32:36.448043Z","iopub.execute_input":"2026-02-01T04:32:36.448434Z","iopub.status.idle":"2026-02-01T04:32:41.891314Z","shell.execute_reply.started":"2026-02-01T04:32:36.448393Z","shell.execute_reply":"2026-02-01T04:32:41.890291Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# 1. The Dataset\n# A tiny sample text (Shakespearian style)\ntext = \"\"\"\nTo be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles\n\"\"\"\n\n# Get all unique characters (our \"vocabulary\")\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nprint(f\"Vocabulary size: {vocab_size}\")\nprint(f\"Vocabulary: {''.join(chars)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T04:33:37.223058Z","iopub.execute_input":"2026-02-01T04:33:37.223413Z","iopub.status.idle":"2026-02-01T04:33:41.817413Z","shell.execute_reply.started":"2026-02-01T04:33:37.223382Z","shell.execute_reply":"2026-02-01T04:33:41.816421Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 27\nVocabulary: \n ',:OTWabdefghiklmnoqrstuw\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Create mappings from character to integer and vice versa\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\n\nencode = lambda s: [stoi[c] for c in s] # Encoder: string -> list of ints\ndecode = lambda l: ''.join([itos[i] for i in l]) # Decoder: list of ints -> string\n\n# Test it\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(f\"First 10 encoded tokens: {data[:10]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T04:33:53.448814Z","iopub.execute_input":"2026-02-01T04:33:53.449412Z","iopub.status.idle":"2026-02-01T04:33:53.509156Z","shell.execute_reply.started":"2026-02-01T04:33:53.449377Z","shell.execute_reply":"2026-02-01T04:33:53.508143Z"}},"outputs":[{"name":"stdout","text":"First 10 encoded tokens: tensor([ 0,  6, 20,  1,  9, 11,  3,  1, 20, 22])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # Each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        # B = Batch (how many parallel sequences)\n        # T = Time (length of the sequence)\n        \n        logits = self.token_embedding_table(idx) # (B,T,C) where C is vocab_size\n\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for CrossEntropyLoss\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            # Calculate how well we predicted the next character\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # Get the predictions\n            logits, loss = self(idx)\n            # Focus only on the last time step (the last character)\n            logits = logits[:, -1, :] # becomes (B, C)\n            # Apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # Append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T04:34:05.873117Z","iopub.execute_input":"2026-02-01T04:34:05.873915Z","iopub.status.idle":"2026-02-01T04:34:05.881592Z","shell.execute_reply.started":"2026-02-01T04:34:05.873883Z","shell.execute_reply":"2026-02-01T04:34:05.880613Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = BigramLanguageModel(vocab_size)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# Training loop\nbatch_size = 32\nblock_size = 8 # Context length\n\nprint(\"\\n--- Starting Training ---\")\nfor steps in range(10000): # High number of steps for such a small dataset\n    \n    # 1. Get a batch of data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    \n    # 2. Forward pass\n    logits, loss = model(x, y)\n    \n    # 3. Backward pass (Update weights)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    if steps % 1000 == 0:\n        print(f\"Step {steps}: Loss {loss.item():.4f}\")\n\nprint(\"--- Training Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T04:34:09.973175Z","iopub.execute_input":"2026-02-01T04:34:09.973468Z","iopub.status.idle":"2026-02-01T04:34:30.705482Z","shell.execute_reply.started":"2026-02-01T04:34:09.973444Z","shell.execute_reply":"2026-02-01T04:34:30.704402Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Training ---\nStep 0: Loss 3.6800\nStep 1000: Loss 2.6164\nStep 2000: Loss 2.0353\nStep 3000: Loss 1.7796\nStep 4000: Loss 1.7054\nStep 5000: Loss 1.6006\nStep 6000: Loss 1.6232\nStep 7000: Loss 1.5756\nStep 8000: Loss 1.5228\nStep 9000: Loss 1.5583\n--- Training Finished ---\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"\\n--- GENERATED TEXT ---\")\n# Start with a single zero token (index 0)\ncontext = torch.zeros((1, 1), dtype=torch.long)\ngenerated_ids = model.generate(context, max_new_tokens=100)\nprint(decode(generated_ids[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T04:34:42.113389Z","iopub.execute_input":"2026-02-01T04:34:42.114220Z","iopub.status.idle":"2026-02-01T04:34:42.144579Z","shell.execute_reply.started":"2026-02-01T04:34:42.114173Z","shell.execute_reply":"2026-02-01T04:34:42.143590Z"}},"outputs":[{"name":"stdout","text":"\n--- GENERATED TEXT ---\n\nWhe, s tha tufe, be offff oblino arthesto in:\nWhe, in:\nWhagageslin:\nThe tofff theor\nWhert ff bleor a\n","output_type":"stream"}],"execution_count":7}]}